
<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>Miipher: A Robust Speech Restoration Model Integrating Self-Supervised Speech and Text Representations</title>
      <link rel="stylesheet" type="text/css" href="stylesheet.css"/>
      <script>
         function play(path) {{
           var player = document.getElementById('player');
           player.src = path;
           player.play();
         }}
      </script>
      <style>
         .audio-cell {
         /* Center audio widgets in the table cell. */
         text-align: center;
         padding-bottom: 1px;
         padding-top: 1px;
         }
         .audio-cell-padded {
         text-align: center;
         padding-bottom: 10px;
         padding-top: 10px;
         }
         .audio-header {
         /* Don't wrap header text. */
         white-space: nowrap;
         /* Some breaking space between headers for readability. */
         padding-right: 5px;
         padding-left: 5px;
         }
         .reference-cell {
         /* For uniformity and to wrap long reference text, limit the reference cell's width. */
         width: 25%;
         padding-top: 20px;
         padding-bottom: 20px;
         }
         .sample audio {
         vertical-align: middle;
         padding-left: 3px;
         padding-right: 3px;
         }
         .round-button {
         box-sizing: border-box;
         display:block;
         width:30px;
         height:30px;
         padding-top: 8px;
         padding-left: 3px;
         line-height: 6px;
         border: 1.2px solid #000;
         border-radius: 50%;
         color: #000;
         text-align:center;
         background-color: rgba(0,0,0,0.00);
         font-size:6px;
         box-shadow: 0px 0px 2px rgba(0,0,0,1);
         transition: all 0.2s ease;
         }
         .round-button:hover {
         background-color: rgba(0,0,0,0.0);
         box-shadow: 0px 0px 4px rgba(0,0,0,1);
         }
         .round-button:active {
         background-color: rgba(0,0,0,0.01);
         box-shadow: 0px 0px 1px rgba(0,0,0,1);
         }
      </style>
   </head>
   <body>
     <div class="main">
       <article>
         <header>
            <h1>Miipher: A Robust Speech Restoration Model Integrating Self-Supervised Speech and Text Representations</h1>
         </header>
      </article>
      <div>
        <p>
		<a href="https://sites.google.com/site/yumakoizumiweb/profile-english">Yuma Koizumi</a>,
	  	<a href="https://research.google/people/HeigaZen/">Heiga Zen</a>,
	  	<a href="https://scholar.google.co.jp/citations?user=enV4FrIAAAAJ">Shigeki Karita</a>,
	  	Yifan Ding,
	  	<a href="https://scholar.google.com/citations?user=dTORL1oAAAAJ">Kohei Yatabe</a>,
	  	<a href="https://scholar.google.com/citations?user=3_2yoWwAAAAJ">Nobuyuki Morioka</a>,
	  	<a href="https://scholar.google.com/citations?user=EilVnKwAAAAJ">Yu Zhang</a>
	  	<a href="https://scholar.google.com/citations?user=RNdGVHoAAAAJ">Wei Han</a>
	  	<a href="https://scholar.google.com/citations?user=6xaz-r0AAAAJ">Ankur Bapna</a>
	  	<a href="http://bacchiani.net/resume/resume.html">Michiel Bacchiani</a>
		</p>
      </div>
      <div>
      <p><b>Abstract: </b>
Speech restoration (SR) is a task of converting degraded speech signals into high-quality ones. In this study, we propose a robust SR model called <I>Miipher</I>, and apply Miipher to a new SR application: increasing the amount of high-quality training data for speech generation by converting speech samples collected from the Web to studio-quality. To make our SR model robust against various degradation, we use (i) a speech representation extracted from w2v-BERT for the input feature, and (ii) a text representation extracted from transcripts via PnG-BERT as a linguistic conditioning feature. Experiments show that Miipher (i) is robust against various audio degradation and (ii) enable us to train a high-quality text-to-speech (TTS) model from restored speech samples collected from the Web.
	  <br /><br />
      <img src="data/overview.png" width="700px" class="center"/> <br />
      </p>



<hr>
<h2 id="intermediate">Comparison on a synthesized dataset:</h2>
<p>
This section shows samples of the objective experiments. We evaluated the effectiveness of w2v-BERT [1] and PnG-BERT [2] while comparing the sound quality between the studio-recorded original speech and restored samples from artificially contaminated samples.
Each table includes the studio-recorded clean target, simulated noisy signal, and outputs of four patterns model using either w2v-BERT features or log-mel spectrogram as the input feature. The four pattern include "full model (i.e. uses both PnG-BERT features and speaker embedding [3])", "w/o text conditioning (i.e. without PnG-BERT)", "w/o speaker embedding", and "w/o text conditioning & speaker embedding".
</p>

<p>

<table>
<tbody>
    <b>Example 1: </b> <I>The Albanian Minister sidestepped the Italian Military Mission, and appointed himself Chief of Staff.</I> <br />
    <tr>
        <th align="center"> </th>
        <th align="center"> Clean target<br /></th>
        <th align="center"> Noisy input<br /></th>
    </tr>
    <tr>
        <th align="center"> </th>
        <td align="center"> <audio controls=""><source src="data/obj1_clean.wav"></audio> <img src="data/obj1_gt.png" width="300" alt=""/></td>
        <td align="center"> <audio controls=""><source src="data/obj1_noisy.wav"></audio> <img src="data/obj1_noisy.png" width="300" alt=""/></td>
    </tr>
    <tr>
        <th align="center"> <br /></th>
    </tr>
    <tr>
        <th align="center"> Feature<br /></th>
        <th align="center"> Full model<br /></th>
        <th align="center"> w/o text conditioning<br /></th>
        <th align="center"> w/o speaker embedding<br /></th>
        <th align="center"> w/o text conditioning & speaker embedding<br /></th>
    </tr>
    <tr>
        <th align="center">w2v-BERT</th>
        <td align="center"> <audio controls=""><source src="data/obj1_w2v.wav"></audio> <img src="data/obj1_w2v_full.png" width="300" alt=""/></td>
        <td align="center"> <audio controls=""><source src="data/obj1_w2v_wo_text.wav"></audio> <img src="data/obj1_w2v_wo_png.png" width="300" alt=""/></td>
        <td align="center"> <audio controls=""><source src="data/obj1_w2v_wo_dvec.wav"></audio> <img src="data/obj1_w2v_wo_dvec.png" width="300" alt=""/></td>
        <td align="center"> <audio controls=""><source src="data/obj1_w2v_wo_text_dvec.wav"></audio> <img src="data/obj1_w2v_wo_png_dvec.png" width="300" alt=""/></td>
    </tr>
    <tr>
        <th align="center">log-mel</th>
        <td align="center"> <audio controls=""><source src="data/obj1_logmel.wav"></audio> <img src="data/obj1_logmel_full.png" width="300" alt=""/></td>
        <td align="center"> <audio controls=""><source src="data/obj1_logmel_wo_text.wav"></audio> <img src="data/obj1_logmel_wo_png.png" width="300" alt=""/></td>
        <td align="center"> <audio controls=""><source src="data/obj1_logmel_wo_dvec.wav"></audio> <img src="data/obj1_logmel_wo_dvec.png" width="300" alt=""/></td>
        <td align="center"> <audio controls=""><source src="data/obj1_logmel_wo_text_dvec.wav"></audio> <img src="data/obj1_logmel_wo_png_dvec.png" width="300" alt=""/></td>
    </tr>
</tbody>
</table><br />

<table>
<tbody>
    <b>Example 2: </b> <I>Thanks again for sharing your story so others can get help sooner than I did.</I> <br />
    <tr>
        <th align="center"> </th>
        <th align="center"> Clean target<br /></th>
        <th align="center"> Noisy input<br /></th>
    </tr>
    <tr>
        <th align="center"> </th>
        <td align="center"> <audio controls=""><source src="data/obj0_clean.wav"></audio> <img src="data/obj0_gt.png" width="300" alt=""/></td>
        <td align="center"> <audio controls=""><source src="data/obj0_noisy.wav"></audio> <img src="data/obj0_noisy.png" width="300" alt=""/></td>
    </tr>
    <tr>
        <th align="center"> <br /></th>
    </tr>
    <tr>
        <th align="center"> Feature<br /></th>
        <th align="center"> Full model<br /></th>
        <th align="center"> w/o text conditioning<br /></th>
        <th align="center"> w/o speaker embedding<br /></th>
        <th align="center"> w/o text conditioning & speaker embedding<br /></th>
    </tr>
    <tr>
        <th align="center">w2v-BERT</th>
        <td align="center"> <audio controls=""><source src="data/obj0_w2v.wav"></audio> <img src="data/obj0_w2v_full.png" width="300" alt=""/></td>
        <td align="center"> <audio controls=""><source src="data/obj0_w2v_wo_text.wav"></audio> <img src="data/obj0_w2v_wo_png.png" width="300" alt=""/></td>
        <td align="center"> <audio controls=""><source src="data/obj0_w2v_wo_dvec.wav"></audio> <img src="data/obj0_w2v_wo_dvec.png" width="300" alt=""/></td>
        <td align="center"> <audio controls=""><source src="data/obj0_w2v_wo_text_dvec.wav"></audio> <img src="data/obj0_w2v_wo_png_dvec.png" width="300" alt=""/></td>
    </tr>
    <tr>
        <th align="center">log-mel</th>
        <td align="center"> <audio controls=""><source src="data/obj0_logmel.wav"></audio> <img src="data/obj0_logmel_full.png" width="300" alt=""/></td>
        <td align="center"> <audio controls=""><source src="data/obj0_logmel_wo_text.wav"></audio> <img src="data/obj0_logmel_wo_png.png" width="300" alt=""/></td>
        <td align="center"> <audio controls=""><source src="data/obj0_logmel_wo_dvec.wav"></audio> <img src="data/obj0_logmel_wo_dvec.png" width="300" alt=""/></td>
        <td align="center"> <audio controls=""><source src="data/obj0_logmel_wo_text_dvec.wav"></audio> <img src="data/obj0_logmel_wo_png_dvec.png" width="300" alt=""/></td>
    </tr>
</tbody>
</table><br /><br />
</p>


<hr>
<h2 id="intermediate">Importance of text-conditioning (force-alignment results):</h2> 

<p>
To demonstrate the effects of text-conditioning in more detail, we performed force-alignment between the restored speech and the ground-truth text. The following examples show that if the input is severely distorted, even using w2v-BERT features, some words cannot be restored, resulting in less accurate force-alignment. Alignment of phonemes and waveforms is especially important for training the acoustic model in a TTS system. Therefore, text-conditioning is very important in Miipher purpose (and perhaps also in robust speech restoration).
</p>


<p>

<table>
<tbody>
    <b>Example 1: </b> <I>Attempting to appease dictatorial regimes with custom beanie babies is a tried and true strategy.</I> <br />
    <tr>
        <th align="center"> </th>
        <th align="center"> 
			Clean target<br />
			<audio controls=""><source src="data/force_align/force_align_ex1_clean.wav"></audio> <br />
			<img src="data/force_align/force_align_ex1_clean.png" width="1200" alt=""/>
		</th>
    </tr>
    <tr>
        <th align="center"> </th>
        <th align="center"> 
			Noisy input<br />
			<audio controls=""><source src="data/force_align/force_align_ex1_noisy.wav"></audio> <br />
			<img src="data/force_align/force_align_ex1_noisy.png" width="1200" alt=""/>
		</th>
    </tr>
    <tr>
        <th align="center"> </th>
        <th align="center"> 
			Miipher output: w2v-BERT <font color="red">with</font> text-conditioning<br />
			<audio controls=""><source src="data/force_align/force_align_ex1_w2vb_full.wav"></audio> <br />
			<img src="data/force_align/force_align_ex1_w2vb_full.png" width="1200" alt=""/>
		</th>
    </tr>
    <tr>
        <th align="center"> </th>
        <th align="center"> 
			Miipher output: w2v-BERT <font color="red">without</font> text-conditioning<br />
			<audio controls=""><source src="data/force_align/force_align_ex1_w2vb_wo_text.wav"></audio> <br />
			<img src="data/force_align/force_align_ex1_w2vb_wo_text.png" width="1200" alt=""/>
		</th>
    </tr>
</tbody>
</table><br />

<table>
<tbody>
    <b>Example 2: </b> <I>The Senate passed a bill that would punish China for keeping its currency artificially low.</I> <br />
    <tr>
        <th align="center"> </th>
        <th align="center"> 
			Clean target<br />
			<audio controls=""><source src="data/force_align/force_align_ex2_clean.wav"></audio> <br />
			<img src="data/force_align/force_align_ex2_clean.png" width="1200" alt=""/>
		</th>
    </tr>
    <tr>
        <th align="center"> </th>
        <th align="center"> 
			Noisy input<br />
			<audio controls=""><source src="data/force_align/force_align_ex2_noisy.wav"></audio> <br />
			<img src="data/force_align/force_align_ex2_noisy.png" width="1200" alt=""/>
		</th>
    </tr>
    <tr>
        <th align="center"> </th>
        <th align="center"> 
			Miipher output: w2v-BERT <font color="red">with</font> text-conditioning<br />
			<audio controls=""><source src="data/force_align/force_align_ex2_w2vb_full.wav"></audio> <br />
			<img src="data/force_align/force_align_ex2_w2vb_full.png" width="1200" alt=""/>
		</th>
    </tr>
    <tr>
        <th align="center"> </th>
        <th align="center"> 
			Miipher output: w2v-BERT <font color="red">without</font> text-conditioning<br />
			<audio controls=""><source src="data/force_align/force_align_ex2_w2vb_wo_text.wav"></audio> <br />
			<img src="data/force_align/force_align_ex2_w2vb_wo_text.png" width="1200" alt=""/>
		</th>
    </tr>
</tbody>
</table><br />

</p>



<hr>
<h2 id="intermediate">Common Voice restoration & TTS outputs:</h2> 

<p>
We applied our SR model to the Common Voice dataset [4] which is an automatic speech recognition (ASR) dataset collected by a crowdsourcing project. We show that our SR model can restore speech samples in-the-wild that contain various types of speech degradation, and enable us to train a TTS model from the restored speech samples.<br><br>

The first block shows the ground-truth samples of the original Common Voice and restored Common Voice.
The second block shows the generated speech of a TTS model trained on the restored Common Voice.
The TTS model consists of a duration unsupervised non-attentive Tacotron (NAT) [5] acoustic model and a WaveRNN neural vocoder [6]. Note that we did not use speaker ID because Common Voice is prohibited from identifying the speaker. That is, it was trained as a single-speaker TTS model.
</p>

<p>
<h3 id="intermediate">Ground-truth example restoration:</h3>
<table>
<tbody>
    <b>Example1: </b> <I>ACCESS MY VIMEO SERVICE TO PLAY MUSIC FROM BERNHARD FLEISCHMANN.</I> <br />
    <tr>
        <td align="center"> Original</td>
        <td align="center"> Restored</td>
    </tr>
    <tr>
        <td align="center"> <audio controls=""><source src="data/cv2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/cv2_r.wav"></audio></td>
    </tr>
</tbody>
</table><br />

<table>
<tbody>
    <b>Example2: </b> <I>CHERYL ASKED ME ABOUT VISITING THE POETRY SLAM WITH HER TOMORROW.</I> <br />
    <tr>
        <td align="center"> Original</td>
        <td align="center"> Restored</td>
    </tr>
    <tr>
        <td align="center"> <audio controls=""><source src="data/cv1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/cv1_r.wav"></audio></td>
    </tr>
</tbody>
</table><br />

<table>
<tbody>
    <b>Example3: </b> <I>THEIR FACES WERE HIDDEN BEHIND BLUE VEILS, WITH ONLY THEIR EYES SHOWING.</I> <br />
    <tr>
        <td align="center"> Original</td>
        <td align="center"> Restored</td>
    </tr>
    <tr>
        <td align="center"> <audio controls=""><source src="data/cv3.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/cv3_r.wav"></audio></td>
    </tr>
</tbody>
</table><br />

<table>
<tbody>
    <b>Example4: </b> <I>I AM GLAD WE HAVE DECIDED THIS THEN.</I> <br />
    <tr>
        <td align="center"> Original</td>
        <td align="center"> Restored</td>
    </tr>
    <tr>
        <td align="center"> <audio controls=""><source src="data/cv4.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/cv4_r.wav"></audio></td>
    </tr>
</tbody>
</table><br />
</p>



<p>
<h3 id="intermediate">TTS using LJspeech sentences:</h3>

<table>
<tbody>
    <b>Example 1: </b> <I>He seems to have felt no responsibility as to the welfare or comfort of those in charge, and out of whom he made all his money.</I> <br />
    <tr>
        <td align="center"> <audio controls=""><source src="data/cv1_tts.wav"></audio></td>
    </tr>
</tbody>
</table><br />

<table>
<tbody>
    <b>Example 2: </b> <I>Prisoners who could afford it sometimes paid for four beds, at the rate of twenty-eight shillings, and so secured the luxury of a private room.</I> <br />
    <tr>
        <td align="center"> <audio controls=""><source src="data/cv2_tts.wav"></audio></td>
    </tr>
</tbody>
</table><br />

<table>
<tbody>
    <b>Example 3: </b> <I>and to pay overseers or instructors out of the county rates.</I> <br />
    <tr>
        <td align="center"> <audio controls=""><source src="data/cv3_tts.wav"></audio></td>
    </tr>
</tbody>
</table><br />

<table>
<tbody>
    <b>Example 4: </b> <I>but no steps were taken to prevent any prisoner from obtaining more if he could pay for it.</I> <br />
    <tr>
        <td align="center"> <audio controls=""><source src="data/cv4_tts.wav"></audio></td>
    </tr>
</tbody>
</table><br />
</p>




<hr>
<h2 id="intermediate">LibriVox restoration & TTS outputs:</h2> 

<p>
As well as the above Common Voice examples, we applied our SR model to speech samples from LibriVox [7]. The restored training dataset contains 13,270 hours of speech samples spoken by 4,000 speakers.
We show that our SR model enables us to train a multi-speaker TTS model from the restored speech samples collected from the web.<br><br>

The first block shows the ground-truth samples of the original LibriVox and restored LibriVox. The second block shows the generated speech of the multi-speaker TTS model. The TTS model and all hyper-parameters were the same as the above Common Voice TTS model except for the number of class for the speaker ID embedding layer.
</p>

<p>
<h3 id="intermediate">Ground-truth example restoration:</h3>

<table>
<tbody>
    <b>Example1: </b> <I>And he, vain and blustering as usual, gave out that he was ready to prove his innocence in person against whomsoever would dare to maintain that he was guilty.</I> <br />
    <tr>
        <td align="center"> Original</td>
        <td align="center"> Restored</td>
    </tr>
    <tr>
        <td align="center"> <audio controls=""><source src="data/vox1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/vox1_r.wav"></audio></td>
    </tr>
</tbody>
</table><br />

<table>
<tbody>
    <b>Example2: </b> <I>His end--for it cannot correctly be called his death--was singular and mysterious.</I> <br />
    <tr>
        <td align="center"> Original</td>
        <td align="center"> Restored</td>
    </tr>
    <tr>
        <td align="center"> <audio controls=""><source src="data/vox4.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/vox4_r.wav"></audio></td>
    </tr>
</tbody>
</table><br />

<table>
<tbody>
    <b>Example3: </b> <I>And the Evidence whereon they were Convicted, stood upon divers particular Circumstances.</I> <br />
    <tr>
        <td align="center"> Original</td>
        <td align="center"> Restored</td>
    </tr>
    <tr>
        <td align="center"> <audio controls=""><source src="data/vox2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/vox2_r.wav"></audio></td>
    </tr>
</tbody>
</table><br />

<table>
<tbody>
    <b>Example4: </b> <I>He also painted a round picture of Our Lady, which is in the Audience Chamber of the Captains of the Guelph party--a very beautiful work.</I> <br />
    <tr>
        <td align="center"> Original</td>
        <td align="center"> Restored</td>
    </tr>
    <tr>
        <td align="center"> <audio controls=""><source src="data/vox3.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/vox3_r.wav"></audio></td>
    </tr>
</tbody>
</table><br />
</p>


<p>
<h3 id="intermediate">TTS using LJspeech sentences:</h3>


<table>
<tbody>
    <b>Example 1: </b> <I>He seems to have felt no responsibility as to the welfare or comfort of those in charge, and out of whom he made all his money.</I> <br />
    <tr>        
        <td align="center"> Speaker ID : 10801</td>
        <td align="center"> Speaker ID : 4788</td>
        <td align="center"> Speaker ID : 8138</td>
        <td align="center"> Speaker ID : 5968</td>
    </tr>
    <tr>
        <td align="center"> <audio controls=""><source src="data/vox1_tts_10801.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/vox1_tts_4788.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/vox1_tts_8138.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/vox1_tts_5968.wav"></audio></td>
    </tr>
</tbody>
</table><br />

<table>
<tbody>
    <b>Example 2: </b> <I>Prisoners who could afford it sometimes paid for four beds, at the rate of twenty-eight shillings, and so secured the luxury of a private room.</I> <br />
    <tr>        
        <td align="center"> Speaker ID : 10801</td>
        <td align="center"> Speaker ID : 4788</td>
        <td align="center"> Speaker ID : 8138</td>
        <td align="center"> Speaker ID : 5968</td>
    </tr>
    <tr>
        <td align="center"> <audio controls=""><source src="data/vox2_tts_10801.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/vox2_tts_4788.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/vox2_tts_8138.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/vox2_tts_5968.wav"></audio></td>
    </tr>
</tbody>
</table><br />

<table>
<tbody>
    <b>Example 3: </b> <I>and to pay overseers or instructors out of the county rates.</I> <br />
    <tr>        
        <td align="center"> Speaker ID : 10801</td>
        <td align="center"> Speaker ID : 4788</td>
        <td align="center"> Speaker ID : 8138</td>
        <td align="center"> Speaker ID : 5968</td>
    </tr>
    <tr>
        <td align="center"> <audio controls=""><source src="data/vox3_tts_10801.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/vox3_tts_4788.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/vox3_tts_8138.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/vox3_tts_5968.wav"></audio></td>
    </tr>
</tbody>
</table><br />

<table>
<tbody>
    <b>Example 4: </b> <I>but no steps were taken to prevent any prisoner from obtaining more if he could pay for it.</I> <br />
    <tr>        
        <td align="center"> Speaker ID : 10801</td>
        <td align="center"> Speaker ID : 4788</td>
        <td align="center"> Speaker ID : 8138</td>
        <td align="center"> Speaker ID : 5968</td>
    </tr>
    <tr>
        <td align="center"> <audio controls=""><source src="data/vox4_tts_10801.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/vox4_tts_4788.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/vox4_tts_8138.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/vox4_tts_5968.wav"></audio></td>
    </tr>
</tbody>
</table><br />
</p>
</p>




<hr>
<h2 id="intermediate">LJspeech restoration & TTS outputs:</h2>

<p>
We applied our SR model to the LJspeech corpus [8] which has been used as a standard TTS dataset. We show that by only changing the training dataset to the restored LJspeech (called LJspeech-R), the quality of TTS outputs improves without changing the model or hyper-parameters.<br><br>

The first two column shows the ground-truth samples of the original LJspeech and LJspeech-R (our SR model outputs).
And the later two columns show the generated speech of TTS models trained on either LJspeech or LJspeech-R.
The TTS model and all hyper-parameters were the same as the above Common Voice TTS model.
</p>

<p>
<table>
<tbody>
    <b>LJ002-0201.wav: </b> <I>He seems to have felt no responsibility as to the welfare or comfort of those in charge, and out of whom he made all his money.</I> <br />
    <tr>
        <td align="center"> LJspeech ground-truth</td>
        <td align="center"> LJspeech-R ground-truth</td>
        <td align="center"> TTS trained on LJSpeech</td>
        <td align="center"> TTS trained on LJSpeech-R</td>
    </tr>
    <tr>
        <td align="center"> <audio controls=""><source src="data/lj1_gt.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/lj1_r_gt.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/lj1_tts.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/lj1_r_tts.wav"></audio></td>
    </tr>
</tbody>
</table><br />

<table>
<tbody>
    <b>LJ003-0169.wav: </b> <I>Prisoners who could afford it sometimes paid for four beds, at the rate of twenty-eight shillings, and so secured the luxury of a private room.</I> <br />
    <tr>
        <td align="center"> LJspeech ground-truth</td>
        <td align="center"> LJspeech-R ground-truth</td>
        <td align="center"> TTS trained on LJSpeech</td>
        <td align="center"> TTS trained on LJSpeech-R</td>
    </tr>
    <tr>
        <td align="center"> <audio controls=""><source src="data/lj2_gt.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/lj2_r_gt.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/lj2_tts.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/lj2_r_tts.wav"></audio></td>
    </tr>
</tbody>
</table><br />

<table>
<tbody>
    <b>LJ004-0089.wav: </b> <I>and to pay overseers or instructors out of the county rates.</I> <br />
    <tr>
        <td align="center"> LJspeech ground-truth</td>
        <td align="center"> LJspeech-R ground-truth</td>
        <td align="center"> TTS trained on LJSpeech</td>
        <td align="center"> TTS trained on LJSpeech-R</td>
    </tr>
    <tr>
        <td align="center"> <audio controls=""><source src="data/lj3_gt.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/lj3_r_gt.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/lj3_tts.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/lj3_r_tts.wav"></audio></td>
    </tr>
</tbody>
</table><br />

<table>
<tbody>
    <b>LJ006-0214.wav: </b> <I>but no steps were taken to prevent any prisoner from obtaining more if he could pay for it.</I> <br />
    <tr>
        <td align="center"> LJspeech ground-truth</td>
        <td align="center"> LJspeech-R ground-truth</td>
        <td align="center"> TTS trained on LJSpeech</td>
        <td align="center"> TTS trained on LJSpeech-R</td>
    </tr>
    <tr>
        <td align="center"> <audio controls=""><source src="data/lj4_gt.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/lj4_r_gt.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/lj4_tts.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/lj4_r_tts.wav"></audio></td>
    </tr>
</tbody>
</table><br />
</p>





<hr>
<h2 id="references">Acknowledgement:</h2>
<p>
We appreciate valuable feedback and support from 
Daniel S. Park,
Hakan Erdogan,
Haruko Ishikawa,
Hynek Hermansky
Johan Schalkwyk,
John R. Hershey,
Keisuke Kinoshita,
Llion Jones,
Neil Zeghidour,
Quan Wang,
Richard William Sproat,
Ron Weiss,
Shiori Yamashita,
Yotaro Kubo, and
Victor Ungureanu.
</p>





<h2 id="references">References:</h2>
<p>
[1] Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, Yonghui Wu,
"w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training,"
In Proc. ASRU, 2021.
[<a href="https://arxiv.org/abs/2108.06209">paper</a>]<br/>

[2] Ye Jia, Heiga Zen, Jonathan Shen, Yu Zhang, Yonghui Wu,
"PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS,"
In Proc. Interspeech, 2021.
[<a href="https://arxiv.org/abs/2103.15060">paper</a>]<br/>

[3] Quan Wang, Yiling Huang, Han Lu, Guanlong Zhao, Ignacio Lopez Moreno,
"Highly Efficient Real-Time Streaming and Fully On-Device Speaker Diarization with Multi-Stage Clustering,"
arXiv:2210.13690, 2022.
[<a href="https://arxiv.org/abs/2210.13690">paper</a>]<br/>

[4] Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M. Tyers, Gregor Weber, "Common Voice: A Massively-Multilingual Speech Corpus," in Proc. LREC, 2020.
[<a href="https://arxiv.org/abs/1912.06670">paper</a>]<br/>

[5] Jonathan Shen, Ye Jia, Mike Chrzanowski, Yu Zhang, Isaac Elias, Heiga Zen, Yonghui Wu, "Non-Attentive Tacotron: Robust and Controllable Neural TTS Synthesis Including Unsupervised Duration Modeling," arXiv:2010.04301, 2020.
[<a href="https://arxiv.org/abs/2010.04301">paper</a>]<br/>

[6] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron van den Oord, Sander Dieleman, Koray Kavukcuoglu
"Efficient Neural Audio Synthesis," in Proc. ICML, 2018
[<a href="https://proceedings.mlr.press/v80/kalchbrenner18a.html">paper</a>]<br/>

[7] <a href="https://librivox.org/">LibriVox web page</a><br/>

[8] Keith Ito, Linda Johnson "The LJ Speech Dataset," 2017.
[<a href="https://keithito.com/LJ-Speech-Dataset/">web page</a>]<br/>
</p>

      </div>
   </body>
</html>
